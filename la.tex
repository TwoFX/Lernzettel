\documentclass[a4paper]{article}

\usepackage[l2tabu, orthodox]{nag}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage[ngerman]{babel}

\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{physics}

\usepackage[framed]{ntheorem}

\usepackage{csquotes}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{enumitem}
\usepackage{stmaryrd}

\usepackage{parskip}

\usepackage[left=2cm, right=2cm, top=2cm, bottom=2.5cm]{geometry}

\newcounter{Sec}

\theoremstyle{marginbreak}
\theorembodyfont{\normalfont}
\newtheorem{definition}{Definition}[Sec]
\newtheorem{satz}[definition]{Satz}
\newtheorem{defsatz}[definition]{Definition und Satz}
\newtheorem{verfahren}[definition]{Verfahren}
\newtheorem{defver}[definition]{Definition und Verfahren}
\newtheorem{defsatzver}[definition]{Definition, Satz und Verfahren}
\newtheorem{satzver}[definition]{Satz und Verfahren}

\MakeOuterQuote{"}

\DeclareMathOperator{\chop}{char}
\DeclareMathOperator{\Kern}{Kern}
\DeclareMathOperator{\Bild}{Bild}
\DeclareMathOperator{\Rang}{Rang}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\sign}{sign}
\newcommand{\sep}{%
	\rule{\textwidth}{0.3pt}%
	\stepcounter{Sec}%
	}
\DeclarePairedDelimiterX\set[1]\lbrace\rbrace{\def\given{\;\delimsize\vert\;}#1}

\begin{document}

	\textsc{Lineare Algebra I}

	\sep
	\begin{definition}[Gruppe]
		Tupel $(G, *)$ mit
		\begin{description}
			\item[G1 (assoziativ)] $\forall a, b, c \in G: (a * b) * c = a * (b * c)$
			\item[G2 (neutrales Element)] $\exists e \in G~\forall a \in G: e * a = a = a * e$
			\item[G3 (inverses Element)] $\forall a \in G~\exists a^{-1} \in G: a^{-1} * a = e = a * a^{-1}$
		\end{description}
	\end{definition}
	\begin{definition}[Abelsche Gruppe]
		\begin{description}
			\item[G4 (kommutativ)] $\forall a, b \in G: a * b = b * a$
		\end{description}
	\end{definition}
	\begin{satz}[Untergruppe]
		\begin{description}
			\item[UG1] $U \neq \varnothing$
			\item[UG2] $\forall a, b \in U: a * b^{-1} \in U$
		\end{description}
	\end{satz}
	\begin{definition}[Erzeugte Gruppe]
		Für $G$ Gruppe und $M \subset G$: $\langle M\rangle \coloneqq \text{Kleinste Untergruppe von $G$, die $M$ enthält}$
	\end{definition}
	\begin{definition}[Zyklische Gruppe]
		Für $a \in G$: $\langle a\rangle \coloneqq \langle\set{a}\rangle$
	\end{definition}
	\sep
	\begin{definition}[Symmetrische Gruppe]
		Menge der bijektiven Selbstabbildungen (Permutationen) einer endlichen Menge $M$.
		Verknüpfung ist die Verkettung $\circ$.
	\end{definition}
	\begin{definition}[Fehlstandszahl]
		Für endliche Menge $M$ und Permutation $\pi$:
		$F(\pi) \coloneqq \abs{\set{1 \leq i, j \leq \abs{M} \given i < j \wedge \pi(i) > \pi(j)}}$

		$\pi~\text{gerade} :\Longleftrightarrow F(\pi)~\text{gerade}$
	\end{definition}
	\begin{satz}[Anzahl Transpositionen]
		Für $\pi$ Permutation und $\tau_1, \ldots, \tau_n$ Transpositionen mit
		$\pi = \tau_1 \circ \ldots \circ \tau_n$: $\pi~\text{gerade} \iff n~\text{gerade}$.
	\end{satz}
	\begin{definition}[Signum]
		$\sign\sigma\coloneqq(-1)^{F(\sigma)}$.
	\end{definition}
	\begin{satz}
		\begin{enumerate}[label=(\alph*)]
			\item Ist $\sigma$ die Verkettung von $r$ Transpositionen, dann $\sign\sigma = (-1)^r$.
			\item $\sign(\sigma_1\circ\sigma_2) = \sign(\sigma_1)\cdot\sign(\sigma_2)$.
			\item $\sign\sigma=\sign\sigma^{-1}$.
		\end{enumerate}
	\end{satz}
	\sep
	\begin{definition}[Ring]
		Tupel $(R, +, \cdot)$ mit
		\begin{description}
			\item[R1] $(R, +)~\text{abelsch}$
			\item[R2] $\cdot~\text{assoziativ}$
			\item[R3] $\forall a, b, c \in R$ gilt: $a\cdot(b + c) = a\cdot b + a\cdot c$ und $(b + c)\cdot a = b\cdot a + c\cdot a$
		\end{description}

		Neutrales Element von $(R, +)$ heißt $0$, inverses Element zu $a \in R$ heißt $-a$, $b - a \coloneqq b + (-a)$.
	\end{definition}
	\begin{definition}[Kommutativer Ring]
		\begin{description}
			\item[R4] $\forall a, b \in R: a \cdot b = b \cdot a$
		\end{description}
	\end{definition}
	\begin{definition}[Ring mit Eins]
		\begin{description}
			\item[R5] $\exists 0 \neq 1 \in R~\forall a \in R: 1 \cdot a = a = a \cdot 1$
		\end{description}
	\end{definition}
	\begin{definition}[Nullteiler]
		Wenn $a, b \in R$, $a \neq 0 \neq b$ und $ab = 0$, dann ist $a$ linker Nullteiler
		und $b$ rechter Nullteiler von $R$.
	\end{definition}
	\sep
	\begin{definition}[Körper]
		$(\mathbb{K}, +, \cdot)~\text{Körper} :\Longleftrightarrow (\mathbb{K}, +, \cdot)~\text{Ring}
		\wedge (\mathbb{K}\setminus\set{0}, \cdot)~\text{abelsch}$
	\end{definition}
	\begin{satz}
		Körper haben keine Nullteiler.
	\end{satz}
	\begin{satz}
		$\mathbb{Z}/n\mathbb{Z}~\text{Körper} \iff n~\text{prim}$
	\end{satz}
	\begin{definition}[Charakteristik]
		\[\chop\mathbb{K} \coloneqq\begin{cases}
			m, &\text{falls ein kleinstes $m$ existiert, sodass $\underbrace{1 + \ldots + 1}_{\text{$m$ mal}} = 0$}\\
			0, & \text{falls kein solches $m$ existiert}
		\end{cases}\]
	\end{definition}
	\begin{satz}
		$\chop\mathbb{K}$ ist entweder 0 oder eine Primzahl.
	\end{satz}
	\sep
	\begin{definition}[Gruppenhomomorphismus]
		Für $(G, *)$, $(H, \circ)$ Gruppen und $\Phi\colon G \to H$:
		\[\Phi~\text{Gruppenhomomorphismus} :\Longleftrightarrow \forall x, y \in G: \Phi(x * y) = \Phi(x)\circ\Phi(y)\]
	\end{definition}
	\begin{definition}[Ringhomomorphismus]
		Für $(R_1, +, \cdot)$, $(R_2, +, \cdot)$ Ringe und $\Phi\colon R_1\to R_2$:
		\[\Phi~\text{Ringhomomorphismus} :\Longleftrightarrow \forall x, y \in R_1: \Phi(x + y) = \Phi(x) + \Phi(y) \wedge
		\Phi(x \cdot y) = \Phi(x) \cdot \Phi(y)\]
	\end{definition}
	\begin{definition}[Körperhomomorphismus]
		Für $(\mathbb{K}_1, +, \cdot)$, $(\mathbb{K}_2, +, \cdot)$ Körper und $\Phi\colon\mathbb{K}_1\to \mathbb{K}_2$:
		\[\Phi~\text{Körperhomomorphismus} :\Longleftrightarrow \forall x, y \in \mathbb{K}_1: \Phi(x + y) = \Phi(x) + \Phi(y) \wedge
		\Phi(x \cdot y) = \Phi(x) \cdot \Phi(y)\]
	\end{definition}
	\begin{definition}[Endomorphismus]
		Homorphismus, der gleichzeitig Selbstabbildung ist
	\end{definition}
	\begin{definition}[Isomorhpismus, Automorphismus]
		Bijektiver Homomorphismus bzw. bijektiver Endomorphismus
	\end{definition}
	\sep
	\begin{definition}[Matrixmultiplikation]
		Für $A = (a_{ij}) \in \mathbb{K}^{p\cross q}$ und $B = (b_{ij}) \in \mathbb{K}^{q\cross r}$
		ist $AB = C = (c_{jk}) \in \mathbb{K}^{p\cross r}$ mit
		\[ c_{jk} \coloneqq \sum_{s = 1}^q a_{js}b_{sk}\]
	\end{definition}
	\begin{defsatz}[Allgemeine lineare Gruppe]
		Die Menge $\mathbf{GL}(n, \mathbb{K})$ der regulären/invertierbaren $n\cross n$-Matrizen ist eine Gruppe.
	\end{defsatz}
	\begin{verfahren}[Inverse Matrix berechnen]
		Um eine $n\cross n$-Matrix zu invertieren, setzt man die $n\cross n$-Einheitsmatrix daneben und wendet
		den Gauß-Algorithmus an.
	\end{verfahren}
	\sep
	\begin{definition}[Grad eines Polynoms]
		\[\deg f\coloneqq\begin{cases}
			n, &\text{falls $a_n \neq 0$ und $a_k = 0$ für alle $k < n$}\\
			-\infty,  &\text{falls $a_k = 0$ für alle $k \geq 0$}
		\end{cases}\]
	\end{definition}
	\begin{satz}
		$(\mathbb{K}[X], +, \cdot)$ ist ein kommutativer Ring mit Eins.
	\end{satz}
	\begin{definition}[Nullstelle eines Polynoms]
		$\alpha\in\mathbb{K}~\text{ist Nullstelle von $p\in\mathbb{K}[X]$} :\Longleftrightarrow f_p(\alpha) = 0$.
	\end{definition}
	\sep
	\begin{definition}[Vektorraum]
		Die Menge $V$ heißt mit den Abbildungen $+\colon V\cross V\to V$, $\cdot\colon\mathbb{K}\cross V\to V$ ein $\mathbb{K}$-Vektorraum, wenn
		\begin{description}
			\item[V1] $(V, +)$ abelsch
			\item[V2] Für alle $\lambda,\mu\in\mathbb{K}$ und $x, y\in V$ gilt:
				\begin{enumerate}[label=(\alph*)]
					\item $1 \cdot x = x$
					\item $\lambda \cdot (\mu \cdot x) = (\lambda \cdot \mu) \cdot x$
					\item $(\lambda + \mu) \cdot x = \lambda \cdot x + \mu \cdot x$
					\item $\lambda \cdot (x + y) = \lambda \cdot x + \lambda \cdot y$
				\end{enumerate}
		\end{description}
	\end{definition}
	\begin{definition}[Lineare Unabhängigkeit]
		Endlich viele Vektoren $v_1,\ldots,v_k \in V$ heißen linear unabhängig, wenn
		\[\sum_{i=1}^k\lambda_iv_i=0\implies\lambda_1=\ldots=\lambda_k=0\]
		Eine unendlich große Menge heißt linear unabhängig, wenn jede endliche Teilmenge
		linear unabhängig ist.
	\end{definition}
	\begin{satz}
		Eine Obermenge einer linear abhängigen Menge ist linear abhängig.
		Eine Teilmenge einer linear unabhängigen Menge ist linear unabhängig.
	\end{satz}
	\begin{satz}
		$k + 1$ Linearkombinationen von $k$ Vektoren sind linear abhängig.
	\end{satz}
	\begin{definition}[Lineare Hülle]
		Für $M \subset V$ ist $[M]$ die Menge der Linearkombinationen von Vektoren aus $M$.
		$[\varnothing] \coloneqq \set{0}$.
	\end{definition}
	\begin{definition}[Erzeugendensystem]
		$M \subset V~\text{erzeugend} :\Longleftrightarrow [M] = V$
	\end{definition}
	\sep
	\begin{defsatz}[Basis]%
		\begin{align*}%
			\text{$B \subset V$ ist Basis von $V$} :\Longleftrightarrow &~\text{$B$ ist erzeugend und linear unabhängig}\\
			\Longleftrightarrow &~\text{$B$ ist erzeugend und minimal}\\
			\Longleftrightarrow &~\text{$B$ ist linear unabhängig und maximal}
		\end{align*}
	\end{defsatz}
	\begin{satz}[Basisergänzungssatz]
		Für Vektorraum $V \neq \set{0}$, $E \subset V$ erzeugend und $L \subset E$ linear unabhängig
		existiert eine Basis $B$ von $V$ mit $L \subset B \subset E$.
	\end{satz}
	\begin{satz}
		Jeder Vektorraum hat eine Basis.
	\end{satz}
	\begin{defsatz}[Dimension]
		Ist $B \subset V$ eine Basis, dann $\dim V \coloneqq \abs{B}$. Die Dimension eines Vektorraums ist eindeutig.
	\end{defsatz}
	\begin{satz}
		Ist $\dim V = n$, dann:
		\begin{enumerate}[label=(\alph*)]
			\item $n + 1$ Vektoren aus $V$ sind linear abhängig.
			\item $n$ linear unabhängige Vektoren sind eine Basis von $V$.
		\end{enumerate}
	\end{satz}
	\sep
	\begin{definition}[Komponentenvektor]
		Ist $b_1, \ldots, b_n \in V$ eine geordnete Basis von $V$, dann hat $v \in V$ die
		eindeutige Basisdarstellung $v = \sum_{i = 1}^nv_ib_i$. Dann
		$\Theta_B(v) \coloneqq (v_1,\ldots,v_n) \in \mathbb{K}^n$.
	\end{definition}
	\begin{defver}[Basiswechsel]
		Für Basen $B, \bar{B}$ heißt die eindeutige Matrix $A$ mit der Eigenschaft
		$\Theta_{\bar{B}}(v) = A\cdot\Theta_B(v)$ für alle $v \in V$ die Übergangsmatrix von
		$B$ nach $\bar{B}$. Die $i$-te Spalte der Basiswechselmatrix sind die Komponenten
		des $i$-ten Basisvektors der Ursprungsbasis bezüglich der neuen Basis, also
		$b_i = a_{1i}\bar{b}_1 + a_{2i}\bar{b}_2 + \ldots + a_{ni}\bar{b}_n$.
	\end{defver}
	\begin{satz}
		Wenn $\Theta_{\bar{B}}(v) = A\cdot\Theta_B(v)$, dann
		$\Theta_B(v) = A^{-1}\cdot\Theta_{\bar{B}}(v)$.
	\end{satz}
	\sep
	\begin{satz}[Untervektorraum]
		\begin{description}
			\item[U1] $U\neq\varnothing$
			\item[U2] $\forall x, y \in U~\forall \lambda\in\mathbb{K}: x + y \in U \wedge \lambda x\in U$
		\end{description}
	\end{satz}
	\begin{satz}
		Für $M \subset V$ ist $[M]$ ein Untervektorraum von $V$.
	\end{satz}
	\begin{satz}
		Für $\varnothing\neq\mathfrak{U}\subset\mathcal{P}(V)$ ist $\bigcap_{U\in\mathfrak{U}}U$
		ein Untervektorraum von U.
	\end{satz}
	\begin{definition}[Summe und direkte Summe]
		$U_1 + U_2 \coloneqq [U_1 \cup U_2]$. Falls $U_1 \cap U_2 = \set{0}$, dann
		$U_1 \oplus U_2 \coloneqq U_1 + U_2$.
	\end{definition}
	\begin{satz}
		\begin{enumerate}[label=(\alph*)]
			\item $U_1 + U_2 = \set{v \in V \given \exists u_1 \in U_1, u_2 \in U_2: v = u_1 + u_2}$
			\item Die Summe ist genau dann direkt, wenn die Wahl von $u_1$ und $u_2$ immer eindeutig ist.
		\end{enumerate}
	\end{satz}
	\begin{satz}[Komplement]
		Für $U_1$ Untervektorraum von V existiert ein Untervektorraum $U_2$ von $V$ mit $U_1 \oplus U_2 = V$.
	\end{satz}
	\sep
	\begin{satz}[Dimension Untervektorraum]
		Ist $V$ endlich-dimensional und $U$ Untervektorraum von $V$, dann $\dim U\leq\dim V$.
		$\dim U=\dim V \iff U = V$.
	\end{satz}
	\begin{satz}[Dimension Summe]
		Ist $V$ endlich-dimensional und $U_1, U_2$ Untervektorräume von $V$, dann
		\[\dim(U_1 + U_2) = \dim U_1 + \dim U_2 - \dim(U_1 \cap U_2).\]
	\end{satz}
	\sep
	\begin{verfahren}[Linear unbhängige Teilmenge finden]
		Vektoren als Spaltenvektoren schreiben, Gauß-Algorithmus anwenden.
		Aus der Treppenform ergibt sich, welche Vektoren entfernt werden können.
	\end{verfahren}
	\begin{verfahren}[Einfache Basis der linearen Hülle finden]
		Vektoren als Zeilenvektoren schreiben, Gauß-Algorithmus anwenden.
		Die Zeilen, die nicht null sind, sind eine Basis der linearen Hülle.
	\end{verfahren}
	\begin{satz}
		Der Zeilenrang und Spaltenrang einer Matrix sind gleich.
	\end{satz}
	\sep
	\begin{definition}[Faktorraum]
		Ist $U$ ein Untervektorraum von $V$, dann ist $V/U$ die Menge der Äquivalenzklassen
		bezüglich der Äquivalenzrelation $x\sim y :\Longleftrightarrow x-y\in U$. Ist $x\in V$,
		dann ist $\tilde{x} = \set{x + u \given u\in U} \eqqcolon x + U$ ein Element von
		$V/U$.
	\end{definition}
	\begin{satz}
		Ist $V$ $n$-dimensional und $U$ ein Untervektorraum von $B$ mit Basis
		$\set{b_1,\ldots,b_d}$, dann lässt sich die Basis zur Basis
		$\set{b_1,\ldots,b_d,b_{d+1},\ldots,b_n}$ von $V$ vervollständigen und $V/U$
		hat die Basis $\set{\tilde{b}_{d+1},\ldots,\tilde{b}_n}$.
	\end{satz}
	\begin{satz}[Dimensionssatz Faktorraum]
		Ist $V$ endlichdimensional und $U$ ein Untervektorraum von $V$, dann gilt
		$\dim V/U = \dim V - \dim U$.
	\end{satz}
	\sep
	\begin{definition}[Lineare Abbildung]
		Für Vektorräume $V$, $W$ heißt $\Phi\colon V\to W$ linear, wenn:
		\begin{description}
			\item[L1] $\forall x, y \in V, \lambda\in\mathbb{K}: \Phi(x + y) = \Phi(x) + \Phi(y)$
			\item[L2] $\forall x, y \in V, \lambda\in\mathbb{K}: \Phi(\lambda x) = \lambda\Phi(x)$
		\end{description}
	\end{definition}
	\begin{satz}
		Für Vektorräume $V$, $W$ ist $\Phi\colon V\to W$ genau dann linear, wenn:
		\begin{description}
			\item[L] $\forall x, y\in V, \lambda, \mu\in\mathbb{K}: \Phi(\lambda x + \mu y) = \lambda\Phi(x) + \mu\Phi(y)$
		\end{description}
	\end{satz}
	\begin{definition}[Vektorraumisomorphie]
		Falls es eine bijektive lineare Abbildungen zwischen Vektorräumen $V$ und $W$
		gibt, dann nennt man diese isomorph und schreibt $V \cong W$.
	\end{definition}
	\sep
	\begin{satz}[Lineare Fortsetzung]
		Für einen Vektorraum $V$, eine Basis ${v_1,\ldots,v_n}$ und Vektoren
		$w_1,\ldots,w_n \in W$ existiert genau eine lineare Abbildung $\Phi\colon V\to W$
		mit $\Phi(v_i) = w_i$ für $ 1 \leq i \leq n$.
	\end{satz}
	\sep
	\begin{satz}
		Bei einer linearen Abbildung gehen linear abhängige Vektoren in linear
		abhängige Vektoren über.
	\end{satz}
	\begin{satz}
		Bei einer injektiven linearen Abbildung gehen linear unabhängige Vektoren in
		linear unabhängige Vektoren über.
	\end{satz}
	\begin{defsatz}[Kern]
		$\Kern\Phi\coloneqq\set{v\in V\given\Phi(v)=0}$ ist ein Untervektorraum von $V$.
	\end{defsatz}
	\begin{satz}
		Eine lineare Abbildung $\Phi\colon V\to W$ ist genau dann injektiv, wenn $\Kern\Phi = \set{0}$.
	\end{satz}
	\begin{defsatz}[Bild]
		$\Bild\Phi\coloneqq\Phi(V)$ ist ein Untervektorraum von $V$.
	\end{defsatz}
	\begin{satz}
		Eine lineare Abbildung $\Phi\colon V\to W$ ist genau dann surjektiv, wenn $\Bild\Phi = W$.
	\end{satz}
	\begin{definition}[Menge der Urbilder]
		$\Phi^{-1}\coloneqq\set{x\in V\given\Phi(x)=w}$.
	\end{definition}
	\begin{satz}[Menge der Mengen der Urbilder ist Faktorraum]
		Für $w\in\Bild\Phi$ gilt $\Phi^{-1}(w) = x + \Kern\Phi \in V/\Kern\Phi$
		für ein $x$ mit $\Phi(x) = w$.
	\end{satz}
	\begin{defsatz}[Kanonische Projektion]
		Für $\Phi\colon V\to W$ ist die Abbildung $\pi\colon V\to V/\Kern\Phi$ mit
		$\pi(x)\coloneqq\tilde{x}$ surjektiv.
	\end{defsatz}
	\begin{satz}[Homomorphiesatz]
		Für $\Phi\colon V\to W$ ist $\tilde{\Phi}\colon V/\Kern\Phi\to W$ mit
		$\tilde{\Phi}(\tilde{x})\coloneqq\Phi(x)$ injektiv und linear und es ist
		$\Phi = \tilde{\Phi}\circ\pi$.
	\end{satz}
	\begin{satz}[Isomorphiesatz]
		Für $\Phi\colon V\to W$ surjektiv ist $\tilde{\Phi}$ surjektiv und somit
		bijektiv. Für allgemeines $\Phi\colon V\to W$ gilt dann $V/\Kern\Phi\cong\Bild\Phi$,
		im surjektiven Fall folgt $V/\Kern\Phi\cong W$.
	\end{satz}
	\sep
	\begin{defsatz}[Rang einer linearen Abbildung]
		Für $\Phi\colon V\to W$ linear ist $\Rang\Phi\coloneqq\dim\Phi(V)=\dim\Bild\Phi\leq\dim W$.
		Ferner $\Rang\Phi\leq\dim V$.
	\end{defsatz}
	\begin{satz}[Dimensionssatz]
		Für $\Phi\colon V\to W$ linear gilt $\Rang\Phi = \dim V - \dim\Kern\Phi$.
	\end{satz}
	\begin{satz}
		\begin{enumerate}[label=(\alph*)]
			\item $\Phi\colon V\to W$ linear ist genau dann injektiv, wenn $\Rang\Phi=\dim V$.
			\item $\Phi\colon V\to W$ linear ist genau dann surjektiv, wenn $\Rang\Phi=\dim W$.
			\item Zwei Vektorräume sind genau dann isomorph, wenn sie die gleiche Dimension haben.
		\end{enumerate}
	\end{satz}
	\sep
	\begin{defsatz}[Menge der linearen Abbildungen]
		Sind $V$ und $W$ $\mathbb{K}$-Vektorräume, so ist die Menge $\Hom(V, W)$ aller
		linearen Abboldungen $\Phi\colon V\to W$ ein $\mathbb{K}$-Vektorraum.
	\end{defsatz}
	\begin{satz}
		Sind $V$ und $W$ endlichdimensional, dann gilt $\dim\Hom(V, W) = \dim V\cdot\dim W$.
	\end{satz}
	\begin{defsatz}[Dualraum]
		Ist $V$ ein $\mathbb{K}$-Vektorraum, dann ist $V^*\coloneqq\Hom(V,\mathbb{K})$.
		Ist $\set{v_1,\ldots,v_n}$ eine Basis von $V$, dann ist $\set{v_1^*,\ldots,v_n^*}$
		mit $v_j^*(v_k)\coloneqq\delta_{jk}$ eine Basis von $V^*$.
	\end{defsatz}
	\begin{defsatz}[Bidualraum]
		Ist $V$ endlichdimensional und $\dim V \geq 1$, dann ist $V^{**}\coloneqq(V^*)^*\cong V$,
		denn $F\colon V\to V^{**}$ mit $F(x)(\Phi)\coloneqq\Phi(x)$ ist ein Vektorraumisomorphismus.
	\end{defsatz}
	\sep
	\begin{defsatzver}[Abbildungsmatrix]
		Die Abbildung $M^B_C\colon\Hom(V, W)\to \mathbb{K}^{m\cross n}$, die jeder
		linearen Abbildung ihre Abbildungsmatrix zuordnet, ist ein Vektorraumisomorphismus.
		Die $k$-te Spalte der Abbildungsmatrix ist $\Theta_C(\Phi(b_k))$.
	\end{defsatzver}
	\begin{satz}
		Für endlichdimensionale $\mathbb{K}$-Vektorräume $V_1$, $V_2$, $V_3$ mit $\dim V_1=l$, $\dim V_2=m$,
		$\dim V_2=n$, Basen $B_1$, $B_2$, $B_3$ und $\Phi\colon V_1\to V_2$ und
		$\Psi\colon V_2\to V_3$ linear gilt $M_{B_3}^{B_1}(\Psi\circ\Phi) = M_{B_3}^{B_2}(\Psi)\cdot M_{B_2}^{B_1}(\Phi)$.
	\end{satz}
	\begin{satz}
		Für endlichdimensionale $\mathbb{K}$-Vektorräume $V$, $W$ mit Basen $B$, $C$ ist
		$\Phi\colon V\to W$ genau dann ein Isomorphismus, wenn $M_C^B(\Phi)$ invertierbar.
		Dann $M_B^C(\Phi^{-1}) = (M_C^B(\Phi))^{-1}$.
	\end{satz}
	\begin{defsatz}[Duale Abbildung]
		Für endlichdimensionale $\mathbb{K}$-Vektorräume $V$, $W$ und $\Phi\colon V\to W$
		linear ist $\Phi^*\colon W^*\to V^*$ mit $\Phi^*(\alpha)\coloneqq\alpha\circ\Phi$
		ebenfalls linear und es gilt $M_{B^*}^{C^*}(\Phi^*) = M_C^B(\Phi)^{\mathsf{T}}$.
	\end{defsatz}
	\begin{defsatz}
		$\hat{\Phi}\colon\mathbb{K}^n\to\mathbb{K}^m$ ist gegeben durch
		$\Theta_C^{-1}\circ\hat{\Phi}\circ\Theta_B=\Phi$ und $\hat{\Phi}=\Theta_C\circ\Phi\circ\Theta_B^{-1}$.
	\end{defsatz}
	\begin{satz}
		Für $\Phi\colon V\to W$ linear und beliebige Basen $B$, $C$ gilt $\Rang\Phi=\Rang M_C^B$.
	\end{satz}
	\sep
	\begin{satz}
		Für $\Phi\colon V\to W$ linear und Basen $B$, $\tilde{B}$ von $V$ und
		$C$, $\tilde{C}$ von $W$ gilt $M_{\tilde{C}}^{\tilde{B}}(\Phi) = M_{\tilde{C}}^C(\id_W)\cdot M_B^C(\Phi)\cdot M_B^{\tilde{B}}(\id_V)$.
	\end{satz}
	\begin{defsatz}[Äquivalente Matrizen]
		Matrizen $A, \tilde{A} \in \mathbb{K}^{m\cross n}$ heißen äquivalent, wenn es
		invertierbare Matrizen $S \in \mathbb{K}^{n\cross n}$ und $T \in \mathbb{K}^{m\cross m}$
		gibt mit $\tilde{A}=TAS$. Äquivalenz ist eine Äquivalenzrelation und elementare
		Zeilen- und Spaltenumformungen geben eine äquivalente Matrix. Zwei Matrizen sind
		genau dann äquivalent, wenn sie den gleichen Rang haben.
	\end{defsatz}
	\begin{definition}[Ähnliche Matrizen]
		Matrizen $A, \tilde{A} \in \mathbb{K}^{n\cross n}$ heißen ähnlich, wenn es eine
		invertierbare Matrix $T\in \mathbb{K}^{n\cross n}$ mit $\tilde{A} = TAT^{-1}$ gibt.
	\end{definition}
	\sep
	\begin{satz}
		Das LGS $Ax = b$ ist genau dann lösbar, wenn $\Rang A = \Rang(A \mid b)$.
	\end{satz}
	\begin{satz}
		Ist $Ax = \Phi(x) = b$ lösbar und ist $x_0$ eine beliebige Lösung, dann
		ist $\mathcal{L} = x_0 + \mathcal{L}_h = x_0 + \Kern\Phi$.
	\end{satz}
	\begin{satz}
		Ein homogenes LGS ist genau dann nichttrivial lösbar, wenn $\Rang A < n$.
		Es ist $\dim\mathcal{L}_h = n - \Rang A$.
	\end{satz}
	\sep
	\begin{defsatz}[Determinante]
		Die Funktion $D$, die geordneten $n$-Tupeln aus $\mathbb{K}^n$ einen Skalar aus
		$\mathbb{K}$ zuordnet und die Eigenschaften
		\begin{description}
			\item[D1] $\forall\lambda\in\mathbb{K},1\leq i\leq n: D(a_1,\ldots,\lambda a_i,\ldots,a_n) = \lambda D(a_1,\ldots,a_i,\ldots,a_n)$
			\item[D2] $D(a_1,\ldots,a_i'+a_i'',\ldots,a_n) = D(a_1,\ldots,a_i',\ldots,a_n) + D(a_1,\ldots,a_i'',\ldots,a_n)$
			\item[D3] Aus $a_i = a_j$ für $i \neq j$ folgt $D(a_1,\ldots,a_i,\ldots,a_j,\ldots,a_n) = 0$
			\item[D4] Für die Standardbassis $e_1,\ldots,e_n$ von $\mathbb{K}^n$ ist $D(e_1,\ldots,e_n) = 1$
		\end{description}
		erfüllt, heißt Determinantenfunktion, existiert und ist eindeutig.
	\end{defsatz}
	\begin{satz}[Eigenschaften der Determinanten]
		\begin{enumerate}[label=(\alph*)]
			\item
				\[ D(a_1,\ldots,\sum_{k=1}^m\lambda_kb_k,\ldots,a_n)=\sum_{k=1}^m\lambda_kD(a_1,\ldots,b_k,\ldots,a_n) \]
			\item
				\[ D(a_1,\ldots,a_i+\sum_{\substack{k=1\\k\neq i}}^m\lambda_ka_k,\ldots,a_n) = D(a_1,\ldots,a_i,\ldots,a_n) \]
			\item
				\[ D(a_1,\ldots,a_i,\ldots,a_j,\ldots,a_n) = -D(a_1,\ldots,a_j,\ldots,a_i,\ldots,a_n) \]
			\item
				\[ D(a_{\sigma(1)},\ldots,a_{\sigma(n)}) = \sign\sigma D(a_1,\ldots,a_n) \]
			\item Die Determinante linear abhängiger Vektoren ist $0$.
		\end{enumerate}
	\end{satz}
	\begin{satz}[Formel von Leibniz]
		\[ D(a_1,\ldots,a_n) = \sum_{\sigma\in S_n}\sign\sigma\cdot a_{1\sigma(1)}\cdots a_{n\sigma(n)} \]
	\end{satz}
	\begin{definition}[Determinante einer quadratischen Matrix]
		$\det A\coloneqq D(a_1,\ldots,a_n)$. Die $a_i$ sind per Definition Zeilenvektoren, das spielt aber keine Rolle.
	\end{definition}
	\begin{satz}
		\begin{enumerate}[label=(\alph*)]
			\item $\det A = \det A^{\mathsf{T}}$
			\item $\det(A\cdot B) = \det A\cdot \det B$
			\item Eine quadratische Matrix $A$ ist genau dann regulär, wenn $\det A\neq 0$. Dann $\det A^{-1} = (\det A)^{-1} = \frac{1}{\det A}$.
		\end{enumerate}
	\end{satz}
	\begin{satzver}[Berechnung der Determinante]
		\begin{enumerate}[label=(\alph*)]
			\item Entwicklung nach der $k$-ten Spalte:
				\[ D(a_1,\ldots,a_n) = \sum_{j=1}^n(-1)^{k+j}a_{jk}D_{jk} \]
			\item Entwicklung nach der $i$-ten Zeile:
				\[ D(a_1,\ldots,a_n) = \sum_{k=1}^n(-1)^{i+k}a_{ik}D_{ik} \]
			\item Beim Vertauschen von zwei Zeilen muss mit $-1$ multipliziert werden.
			\item Wird eine Zeile mit $k$ multipliziert, muss mit $k^{-1}$ multipliziert werden.
			\item Wird das Vielfache einer Zeile auf eine andere Zeile addiert, wird nicht multipliziert.
		\end{enumerate}
	\end{satzver}
	\sep
	\begin{satz}
		Ein homogenes LGS $Ax=0$ hat genau dann nur die triviale Lösung, wenn $\det A\neq0$.
	\end{satz}
	\begin{satz}[Kramersche Regel]
		Ein LGS $Ax=b$ mit $\det A\neq 0$ hat genau eine Lösung $x = (x_1,\ldots,x_n)$ und es gilt
		$x_k = \frac{D_k}{\det A}$, wobei $D_k\coloneqq\det(a_{\cdot1}\cdots a_{\cdot k} \mid b \mid a_{\cdot k+1}\cdots a_{\cdot n})$.
	\end{satz}
	\sep
	\begin{definition}[Eigenwert]
		Für $\Phi\colon V\to V$ heißt $\lambda\in\mathbb{K}$ Eigenwert, wenn $0\neq x\in V$
		existiert mit $\Phi(x)=\lambda x \iff (\Phi-\lambda\id_V)(x)=0$.
	\end{definition}
	\begin{definition}[Eigenraum]
		$E_\lambda\coloneqq\set{x\given\Phi(x)=\lambda x}=\Kern(\Phi-\lambda\id_V)$
	\end{definition}
	\begin{definition}[Spektrum]
		$\set{\lambda\given\exists x\neq0:\Phi(x)=\lambda x}$ heißt Spektrum von $\Phi$.
	\end{definition}
	\begin{satz}
		Zu verschiedenen Eigenwerten gehörende Eigenvektoren eines Endomorphismus sind linear unabhängig.
		Ein Endomorphismus hat somit maximal $n$ Eigenwerte.
	\end{satz}
	\begin{definition}[Charakteristisches Polynom]
		$p_A\coloneqq\det(A-XE_n)$
	\end{definition}
	\begin{satzver}[Bestimmung der Eigenwerte]
		$\lambda~\text{ist Eigenwert von $A \in\mathbb{K}^{n\cross n}$} \iff \det(A-\lambda E_n) = 0 \iff \lambda~\text{ist Nullstelle von $p_A$}$.
	\end{satzver}
	\begin{satzver}[Bestimmung des Eigenraums]
		$E_\lambda$ entspricht der Lösungsmenge des homogenen LGS $(A-\lambda E_n)x = 0$.
	\end{satzver}
	\sep
	\begin{definition}[Diagonalisierbar]
		Eine quadratische Matrix heißt diagonalisierbar, wenn sie zu einer Diagonalmatrix
		ähnlich ist. Ein Endomorphismus heißt diagonalisierbar, wenn er eine Abbildungsmatrix
		in Diagonalgestalt hat.
	\end{definition}
	\begin{definition}[Geometrische Vielfalt]
		Die Geometrische Vielfalt eines Eigenvektors $\lambda$ ist $\dim\Bild(\Phi-\lambda\id_V)$.
	\end{definition}
	\begin{definition}[Linearfaktoren und algebraische Vielfalt]
		Ein $p_A$ zerfällt in Linearfaktoren, wenn es sich
		in der Form $p_A=(-1)^n(X-\lambda_1)^{r_1}\cdots(X-\lambda_k)^{r_k}$ schreiben
		lässt. $r_i$ ist dann die algebraische Vielfalt des Eigenvektors $\lambda_i$.
	\end{definition}
	\begin{satz}
		Die folgenden Aussagen sind äquivalent.
		\begin{enumerate}[label=(\alph*)]
			\item $\Phi$ ist diagonalisierbar.
			\item In $V$ gibt es eine Basis aus Eigenvektoren von $\Phi$.
			\item $V$ ist die direkte Summe der Eigenräume von $\Phi$.
			\item Die Summe der Dimensionen der Eigenräume von $\Phi$ ist $n$.
			\item Daas charakteristische Polynom zerfällt in Linearfaktoren und
			für alle Eigenwerte sind die geometrische und algebraische Vielfalt gleich.
		\end{enumerate}
	\end{satz}
	\begin{defsatz}[Trigonalisierbarkeit]
		Ein Endomorphismus heißt trigonalisierbar, wenn er eine Abbildungsmatrix hat,
		die eine obere Dreiecksmatrix ist. Dies ist genau dann der Fall, wenn das
		charakterisitische Polynom in Linearfaktoren zerfällt.
	\end{defsatz}
	\sep
	\begin{satz}[Satz von Cayley-Hamilton]
		Für einen Endomorphismus $\Phi$ ist $p_\Phi(\Phi) = 0$, also $\forall v\in V: p_\Phi(\Phi)(v) = 0$.
	\end{satz}
\end{document}
