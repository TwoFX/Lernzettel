\documentclass[a4paper]{article}

\usepackage[l2tabu, orthodox]{nag}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage[ngerman]{babel}

\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{physics}

\usepackage[framed]{ntheorem}

\usepackage{csquotes}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{enumitem}
\usepackage{stmaryrd}

\usepackage{parskip}

\usepackage[left=1.8cm, right=1.8cm, top=1.8cm, bottom=2.5cm]{geometry}

\newcounter{Sec}

\theoremstyle{marginbreak}
\theorembodyfont{\normalfont}
\newtheorem{definition}{Definition}[Sec]
\newtheorem{satz}[definition]{Satz}
\newtheorem{defsatz}[definition]{Definition und Satz}
\newtheorem{verfahren}[definition]{Verfahren}
\newtheorem{defver}[definition]{Definition und Verfahren}
\newtheorem{defsatzver}[definition]{Definition, Satz und Verfahren}
\newtheorem{satzver}[definition]{Satz und Verfahren}

\MakeOuterQuote{"}

\DeclareMathOperator{\chop}{char}
\DeclareMathOperator{\Kern}{Kern}
\DeclareMathOperator{\Bild}{Bild}
\DeclareMathOperator{\Rang}{Rang}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\Spek}{Spek}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\diag}{diag}
\newcommand{\sep}{%
	\rule{\textwidth}{0.3pt}%
	\stepcounter{Sec}%
	}
\DeclarePairedDelimiterX\set[1]\lbrace\rbrace{\def\given{\;\delimsize\vert\;}#1}

\newcommand\conj{\overline}
\newcommand\scp[1]{\left\langle#1\right\rangle}
\newcommand\transpose[1]{#1^{\mathsf{T}}}

\begin{document}

	\textsc{Lineare Algebra I}

	\sep
	\begin{definition}[Gruppe]
		Tupel $(G, *)$ mit
		\begin{description}
			\item[G1 (assoziativ)] $\forall a, b, c \in G: (a * b) * c = a * (b * c)$
			\item[G2 (neutrales Element)] $\exists e \in G~\forall a \in G: e * a = a = a * e$
			\item[G3 (inverses Element)] $\forall a \in G~\exists a^{-1} \in G: a^{-1} * a = e = a * a^{-1}$
		\end{description}
	\end{definition}
	\begin{definition}[Abelsche Gruppe]
		\begin{description}
			\item[G4 (kommutativ)] $\forall a, b \in G: a * b = b * a$
		\end{description}
	\end{definition}
	\begin{satz}[Untergruppe]
		\begin{description}
			\item[UG1] $U \neq \varnothing$
			\item[UG2] $\forall a, b \in U: a * b^{-1} \in U$
		\end{description}
	\end{satz}
	\begin{definition}[Erzeugte Gruppe]
		Für $G$ Gruppe und $M \subset G$: $\langle M\rangle \coloneqq \text{Kleinste Untergruppe von $G$, die $M$ enthält}$
	\end{definition}
	\begin{definition}[Zyklische Gruppe]
		Für $a \in G$: $\langle a\rangle \coloneqq \langle\set{a}\rangle$
	\end{definition}
	\sep
	\begin{definition}[Symmetrische Gruppe]
		Menge der bijektiven Selbstabbildungen (Permutationen) einer endlichen Menge $M$.
		Verknüpfung ist die Verkettung $\circ$.
	\end{definition}
	\begin{definition}[Fehlstandszahl]
		Für endliche Menge $M$ und Permutation $\pi$:
		$F(\pi) \coloneqq \abs{\set{1 \leq i, j \leq \abs{M} \given i < j \wedge \pi(i) > \pi(j)}}$

		$\pi~\text{gerade} :\Longleftrightarrow F(\pi)~\text{gerade}$
	\end{definition}
	\begin{satz}[Anzahl Transpositionen]
		Für $\pi$ Permutation und $\tau_1, \ldots, \tau_n$ Transpositionen mit
		$\pi = \tau_1 \circ \ldots \circ \tau_n$: $\pi~\text{gerade} \iff n~\text{gerade}$.
	\end{satz}
	\begin{definition}[Signum]
		$\sign\sigma\coloneqq(-1)^{F(\sigma)}$.
	\end{definition}
	\begin{satz}
		\begin{enumerate}[label=(\alph*)]
			\item Ist $\sigma$ die Verkettung von $r$ Transpositionen, dann $\sign\sigma = (-1)^r$.
			\item $\sign(\sigma_1\circ\sigma_2) = \sign(\sigma_1)\cdot\sign(\sigma_2)$.
			\item $\sign\sigma=\sign\sigma^{-1}$.
		\end{enumerate}
	\end{satz}
	\sep
	\begin{definition}[Ring]
		Tupel $(R, +, \cdot)$ mit
		\begin{description}
			\item[R1] $(R, +)~\text{abelsch}$
			\item[R2] $\cdot~\text{assoziativ}$
			\item[R3] $\forall a, b, c \in R$ gilt: $a\cdot(b + c) = a\cdot b + a\cdot c$ und $(b + c)\cdot a = b\cdot a + c\cdot a$
		\end{description}

		Neutrales Element von $(R, +)$ heißt $0$, inverses Element zu $a \in R$ heißt $-a$, $b - a \coloneqq b + (-a)$.
	\end{definition}
	\begin{definition}[Kommutativer Ring]
		\begin{description}
			\item[R4] $\forall a, b \in R: a \cdot b = b \cdot a$
		\end{description}
	\end{definition}
	\begin{definition}[Ring mit Eins]
		\begin{description}
			\item[R5] $\exists 0 \neq 1 \in R~\forall a \in R: 1 \cdot a = a = a \cdot 1$
		\end{description}
	\end{definition}
	\begin{definition}[Nullteiler]
		Wenn $a, b \in R$, $a \neq 0 \neq b$ und $ab = 0$, dann ist $a$ linker Nullteiler
		und $b$ rechter Nullteiler von $R$.
	\end{definition}
	\sep
	\begin{definition}[Körper]
		$(\mathbb{K}, +, \cdot)~\text{Körper} :\Longleftrightarrow (\mathbb{K}, +, \cdot)~\text{Ring}
		\wedge (\mathbb{K}\setminus\set{0}, \cdot)~\text{abelsch}$
	\end{definition}
	\begin{satz}
		Körper haben keine Nullteiler.
	\end{satz}
	\begin{satz}
		$\mathbb{Z}/n\mathbb{Z}~\text{Körper} \iff n~\text{prim}$
	\end{satz}
	\begin{definition}[Charakteristik]
		\[\chop\mathbb{K} \coloneqq\begin{cases}
			m, &\text{falls ein kleinstes $m$ existiert, sodass $\underbrace{1 + \ldots + 1}_{\text{$m$ mal}} = 0$}\\
			0, & \text{falls kein solches $m$ existiert}
		\end{cases}\]
	\end{definition}
	\begin{satz}
		$\chop\mathbb{K}$ ist entweder $0$ oder eine Primzahl.
	\end{satz}
	\sep
	\begin{definition}[Gruppenhomomorphismus]
		Für $(G, *)$, $(H, \circ)$ Gruppen und $\Phi\colon G \to H$:
		\[\Phi~\text{Gruppenhomomorphismus} :\Longleftrightarrow \forall x, y \in G: \Phi(x * y) = \Phi(x)\circ\Phi(y)\]
	\end{definition}
	\begin{definition}[Ringhomomorphismus]
		Für $(R_1, +, \cdot)$, $(R_2, +, \cdot)$ Ringe und $\Phi\colon R_1\to R_2$:
		\[\Phi~\text{Ringhomomorphismus} :\Longleftrightarrow \forall x, y \in R_1: \Phi(x + y) = \Phi(x) + \Phi(y) \wedge
		\Phi(x \cdot y) = \Phi(x) \cdot \Phi(y)\]
	\end{definition}
	\begin{definition}[Körperhomomorphismus]
		Für $(\mathbb{K}_1, +, \cdot)$, $(\mathbb{K}_2, +, \cdot)$ Körper und $\Phi\colon\mathbb{K}_1\to \mathbb{K}_2$:
		\[\Phi~\text{Körperhomomorphismus} :\Longleftrightarrow \forall x, y \in \mathbb{K}_1: \Phi(x + y) = \Phi(x) + \Phi(y) \wedge
		\Phi(x \cdot y) = \Phi(x) \cdot \Phi(y)\]
	\end{definition}
	\begin{definition}[Endomorphismus]
		Homorphismus, der gleichzeitig Selbstabbildung ist
	\end{definition}
	\begin{definition}[Isomorhpismus, Automorphismus]
		Bijektiver Homomorphismus bzw. bijektiver Endomorphismus
	\end{definition}
	\sep
	\begin{definition}[Matrixmultiplikation]
		Für $A = (a_{ij}) \in \mathbb{K}^{p\times q}$ und $B = (b_{ij}) \in \mathbb{K}^{q\times r}$
		ist $AB = C = (c_{jk}) \in \mathbb{K}^{p\times r}$ mit
		\[ c_{jk} \coloneqq \sum_{s = 1}^q a_{js}b_{sk}\]
	\end{definition}
	\begin{defsatz}[Allgemeine lineare Gruppe]
		Die Menge $\mathbf{GL}(n, \mathbb{K})$ der regulären/invertierbaren $n\times n$-Matrizen ist eine Gruppe.
	\end{defsatz}
	\begin{verfahren}[Inverse Matrix berechnen]
		Um eine $n\times n$-Matrix zu invertieren, setzt man die $n\times n$-Einheitsmatrix daneben und wendet
		den Gauß-Algorithmus an.
	\end{verfahren}
	\sep
	\begin{definition}[Grad eines Polynoms]
		\[\deg f\coloneqq\begin{cases}
			n, &\text{falls $a_n \neq 0$ und $a_k = 0$ für alle $k < n$}\\
			-\infty,  &\text{falls $a_k = 0$ für alle $k \geq 0$}
		\end{cases}\]
	\end{definition}
	\begin{satz}
		$(\mathbb{K}[X], +, \cdot)$ ist ein kommutativer Ring mit Eins.
	\end{satz}
	\begin{definition}[Nullstelle eines Polynoms]
		$\alpha\in\mathbb{K}~\text{ist Nullstelle von $p\in\mathbb{K}[X]$} :\Longleftrightarrow f_p(\alpha) = 0$.
	\end{definition}
	\sep
	\begin{definition}[Vektorraum]
		Die Menge $V$ heißt mit den Abbildungen $+\colon V\times V\to V$, $\cdot\colon\mathbb{K}\times V\to V$ ein $\mathbb{K}$-Vektorraum, wenn
		\begin{description}
			\item[V1] $(V, +)$ abelsch
			\item[V2] Für alle $\lambda,\mu\in\mathbb{K}$ und $x, y\in V$ gilt:
				\begin{enumerate}[label=(\alph*)]
					\item $1 \cdot x = x$
					\item $\lambda \cdot (\mu \cdot x) = (\lambda \cdot \mu) \cdot x$
					\item $(\lambda + \mu) \cdot x = \lambda \cdot x + \mu \cdot x$
					\item $\lambda \cdot (x + y) = \lambda \cdot x + \lambda \cdot y$
				\end{enumerate}
		\end{description}
	\end{definition}
	\begin{definition}[Lineare Unabhängigkeit]
		Endlich viele Vektoren $v_1,\ldots,v_k \in V$ heißen linear unabhängig, wenn
		\[\sum_{i=1}^k\lambda_iv_i=0\implies\lambda_1=\ldots=\lambda_k=0\]
		Eine unendlich große Menge heißt linear unabhängig, wenn jede endliche Teilmenge
		linear unabhängig ist.
	\end{definition}
	\begin{satz}
		Eine Obermenge einer linear abhängigen Menge ist linear abhängig.
		Eine Teilmenge einer linear unabhängigen Menge ist linear unabhängig.
	\end{satz}
	\begin{satz}
		$k + 1$ Linearkombinationen von $k$ Vektoren sind linear abhängig.
	\end{satz}
	\begin{definition}[Lineare Hülle]
		Für $M \subset V$ ist $[M]$ die Menge der Linearkombinationen von Vektoren aus $M$.
		$[\varnothing] \coloneqq \set{0}$.
	\end{definition}
	\begin{definition}[Erzeugendensystem]
		$M \subset V~\text{erzeugend} :\Longleftrightarrow [M] = V$
	\end{definition}
	\sep
	\begin{defsatz}[Basis]%
		\begin{align*}%
			\text{$B \subset V$ ist Basis von $V$} :\Longleftrightarrow &~\text{$B$ ist erzeugend und linear unabhängig}\\
			\Longleftrightarrow &~\text{$B$ ist erzeugend und minimal}\\
			\Longleftrightarrow &~\text{$B$ ist linear unabhängig und maximal}
		\end{align*}
	\end{defsatz}
	\begin{satz}[Basisergänzungssatz]
		Für Vektorraum $V \neq \set{0}$, $E \subset V$ erzeugend und $L \subset E$ linear unabhängig
		existiert eine Basis $B$ von $V$ mit $L \subset B \subset E$.
	\end{satz}
	\begin{satz}
		Jeder Vektorraum hat eine Basis.
	\end{satz}
	\begin{defsatz}[Dimension]
		Ist $B \subset V$ eine Basis, dann $\dim V \coloneqq \abs{B}$. Die Dimension eines Vektorraums ist eindeutig.
	\end{defsatz}
	\begin{satz}
		Ist $\dim V = n$, dann:
		\begin{enumerate}[label=(\alph*)]
			\item $n + 1$ Vektoren aus $V$ sind linear abhängig.
			\item $n$ linear unabhängige Vektoren sind eine Basis von $V$.
		\end{enumerate}
	\end{satz}
	\sep
	\begin{definition}[Komponentenvektor]
		Ist $b_1, \ldots, b_n \in V$ eine geordnete Basis von $V$, dann hat $v \in V$ die
		eindeutige Basisdarstellung $v = \sum_{i = 1}^nv_ib_i$. Dann
		$\Theta_B(v) \coloneqq (v_1,\ldots,v_n) \in \mathbb{K}^n$.
	\end{definition}
	\begin{defver}[Basiswechsel]
		Für Basen $B, \bar{B}$ heißt die eindeutige Matrix $A$ mit der Eigenschaft
		$\Theta_{\bar{B}}(v) = A\cdot\Theta_B(v)$ für alle $v \in V$ die Übergangsmatrix von
		$B$ nach $\bar{B}$. Die $i$-te Spalte der Basiswechselmatrix sind die Komponenten
		des $i$-ten Basisvektors der Ursprungsbasis bezüglich der neuen Basis, also
		$b_i = a_{1i}\bar{b}_1 + a_{2i}\bar{b}_2 + \ldots + a_{ni}\bar{b}_n$.
	\end{defver}
	\begin{satz}
		Wenn $\Theta_{\bar{B}}(v) = A\cdot\Theta_B(v)$, dann
		$\Theta_B(v) = A^{-1}\cdot\Theta_{\bar{B}}(v)$.
	\end{satz}
	\sep
	\begin{satz}[Untervektorraum]
		\begin{description}
			\item[U1] $U\neq\varnothing$
			\item[U2] $\forall x, y \in U~\forall \lambda\in\mathbb{K}: x + y \in U \wedge \lambda x\in U$
		\end{description}
	\end{satz}
	\begin{satz}
		Für $M \subset V$ ist $[M]$ ein Untervektorraum von $V$.
	\end{satz}
	\begin{satz}
		Für $\varnothing\neq\mathfrak{U}\subset\mathcal{P}(V)$ ist $\bigcap_{U\in\mathfrak{U}}U$
		ein Untervektorraum von U.
	\end{satz}
	\begin{definition}[Summe und direkte Summe]
		$U_1 + U_2 \coloneqq [U_1 \cup U_2]$. Falls $U_1 \cap U_2 = \set{0}$, dann
		$U_1 \oplus U_2 \coloneqq U_1 + U_2$.
	\end{definition}
	\begin{satz}
		\begin{enumerate}[label=(\alph*)]
			\item $U_1 + U_2 = \set{v \in V \given \exists u_1 \in U_1, u_2 \in U_2: v = u_1 + u_2}$
			\item Die Summe ist genau dann direkt, wenn die Wahl von $u_1$ und $u_2$ immer eindeutig ist.
		\end{enumerate}
	\end{satz}
	\begin{satz}[Komplement]
		Für $U_1$ Untervektorraum von V existiert ein Untervektorraum $U_2$ von $V$ mit $U_1 \oplus U_2 = V$.
	\end{satz}
	\sep
	\begin{satz}[Dimension Untervektorraum]
		Ist $V$ endlich-dimensional und $U$ Untervektorraum von $V$, dann $\dim U\leq\dim V$.
		$\dim U=\dim V \iff U = V$.
	\end{satz}
	\begin{satz}[Dimension Summe]
		Ist $V$ endlich-dimensional und $U_1, U_2$ Untervektorräume von $V$, dann
		\[\dim(U_1 + U_2) = \dim U_1 + \dim U_2 - \dim(U_1 \cap U_2).\]
	\end{satz}
	\sep
	\begin{verfahren}[Linear unbhängige Teilmenge finden]
		Vektoren als Spaltenvektoren schreiben, Gauß-Algorithmus anwenden.
		Aus der Treppenform ergibt sich, welche Vektoren entfernt werden können.
	\end{verfahren}
	\begin{verfahren}[Einfache Basis der linearen Hülle finden]
		Vektoren als Zeilenvektoren schreiben, Gauß-Algorithmus anwenden.
		Die Zeilen, die nicht null sind, sind eine Basis der linearen Hülle.
	\end{verfahren}
	\begin{satz}
		Der Zeilenrang und Spaltenrang einer Matrix sind gleich.
	\end{satz}
	\sep
	\begin{definition}[Faktorraum]
		Ist $U$ ein Untervektorraum von $V$, dann ist $V/U$ die Menge der Äquivalenzklassen
		bezüglich der Äquivalenzrelation $x\sim y :\Longleftrightarrow x-y\in U$. Ist $x\in V$,
		dann ist $\tilde{x} = \set{x + u \given u\in U} \eqqcolon x + U$ ein Element von
		$V/U$.
	\end{definition}
	\begin{satz}
		Ist $V$ $n$-dimensional und $U$ ein Untervektorraum von $B$ mit Basis
		$\set{b_1,\ldots,b_d}$, dann lässt sich die Basis zur Basis
		$\set{b_1,\ldots,b_d,b_{d+1},\ldots,b_n}$ von $V$ vervollständigen und $V/U$
		hat die Basis $\set{\tilde{b}_{d+1},\ldots,\tilde{b}_n}$.
	\end{satz}
	\begin{satz}[Dimensionssatz Faktorraum]
		Ist $V$ endlichdimensional und $U$ ein Untervektorraum von $V$, dann gilt
		$\dim V/U = \dim V - \dim U$.
	\end{satz}
	\sep
	\begin{definition}[Lineare Abbildung]
		Für Vektorräume $V$, $W$ heißt $\Phi\colon V\to W$ linear, wenn:
		\begin{description}
			\item[L1] $\forall x, y \in V, \lambda\in\mathbb{K}: \Phi(x + y) = \Phi(x) + \Phi(y)$
			\item[L2] $\forall x, y \in V, \lambda\in\mathbb{K}: \Phi(\lambda x) = \lambda\Phi(x)$
		\end{description}
	\end{definition}
	\begin{satz}
		Für Vektorräume $V$, $W$ ist $\Phi\colon V\to W$ genau dann linear, wenn:
		\begin{description}
			\item[L] $\forall x, y\in V, \lambda, \mu\in\mathbb{K}: \Phi(\lambda x + \mu y) = \lambda\Phi(x) + \mu\Phi(y)$
		\end{description}
	\end{satz}
	\begin{definition}[Vektorraumisomorphie]
		Falls es eine bijektive lineare Abbildungen zwischen Vektorräumen $V$ und $W$
		gibt, dann nennt man diese isomorph und schreibt $V \cong W$.
	\end{definition}
	\sep
	\begin{satz}[Lineare Fortsetzung]
		Für einen Vektorraum $V$, eine Basis ${v_1,\ldots,v_n}$ und Vektoren
		$w_1,\ldots,w_n \in W$ existiert genau eine lineare Abbildung $\Phi\colon V\to W$
		mit $\Phi(v_i) = w_i$ für $ 1 \leq i \leq n$.
	\end{satz}
	\sep
	\begin{satz}
		Bei einer linearen Abbildung gehen linear abhängige Vektoren in linear
		abhängige Vektoren über.
	\end{satz}
	\begin{satz}
		Bei einer injektiven linearen Abbildung gehen linear unabhängige Vektoren in
		linear unabhängige Vektoren über.
	\end{satz}
	\begin{defsatz}[Kern]
		$\Kern\Phi\coloneqq\set{v\in V\given\Phi(v)=0}$ ist ein Untervektorraum von $V$.
	\end{defsatz}
	\begin{satz}
		Eine lineare Abbildung $\Phi\colon V\to W$ ist genau dann injektiv, wenn $\Kern\Phi = \set{0}$.
	\end{satz}
	\begin{defsatz}[Bild]
		$\Bild\Phi\coloneqq\Phi(V)$ ist ein Untervektorraum von $V$.
	\end{defsatz}
	\begin{satz}
		Eine lineare Abbildung $\Phi\colon V\to W$ ist genau dann surjektiv, wenn $\Bild\Phi = W$.
	\end{satz}
	\begin{definition}[Menge der Urbilder]
		$\Phi^{-1}(w)\coloneqq\set{x\in V\given\Phi(x)=w}$.
	\end{definition}
	\begin{satz}[Menge der Mengen der Urbilder ist Faktorraum]
		Für $w\in\Bild\Phi$ gilt $\Phi^{-1}(w) = x + \Kern\Phi \in V/\Kern\Phi$
		für ein $x$ mit $\Phi(x) = w$.
	\end{satz}
	\begin{defsatz}[Kanonische Projektion]
		Für $\Phi\colon V\to W$ ist die Abbildung $\pi\colon V\to V/\Kern\Phi$ mit
		$\pi(x)\coloneqq\tilde{x}$ surjektiv.
	\end{defsatz}
	\begin{satz}[Homomorphiesatz]
		Für $\Phi\colon V\to W$ ist $\tilde{\Phi}\colon V/\Kern\Phi\to W$ mit
		$\tilde{\Phi}(\tilde{x})\coloneqq\Phi(x)$ injektiv und linear und es ist
		$\Phi = \tilde{\Phi}\circ\pi$.
	\end{satz}
	\begin{satz}[Isomorphiesatz]
		Für $\Phi\colon V\to W$ surjektiv ist $\tilde{\Phi}$ surjektiv und somit
		bijektiv. Für allgemeines $\Phi\colon V\to W$ gilt dann $V/\Kern\Phi\cong\Bild\Phi$,
		im surjektiven Fall folgt $V/\Kern\Phi\cong W$.
	\end{satz}
	\sep
	\begin{defsatz}[Rang einer linearen Abbildung]
		Für $\Phi\colon V\to W$ linear ist $\Rang\Phi\coloneqq\dim\Phi(V)=\dim\Bild\Phi\leq\dim W$.
		Ferner $\Rang\Phi\leq\dim V$.
	\end{defsatz}
	\begin{satz}[Dimensionssatz]
		Für $\Phi\colon V\to W$ linear gilt $\Rang\Phi = \dim V - \dim\Kern\Phi$.
	\end{satz}
	\begin{satz}
		\begin{enumerate}[label=(\alph*)]
			\item $\Phi\colon V\to W$ linear ist genau dann injektiv, wenn $\Rang\Phi=\dim V$.
			\item $\Phi\colon V\to W$ linear ist genau dann surjektiv, wenn $\Rang\Phi=\dim W$.
			\item Zwei Vektorräume sind genau dann isomorph, wenn sie die gleiche Dimension haben.
		\end{enumerate}
	\end{satz}
	\sep
	\begin{defsatz}[Menge der linearen Abbildungen]
		Sind $V$ und $W$ $\mathbb{K}$-Vektorräume, so ist die Menge $\Hom(V, W)$ aller
		linearen Abbildungen $\Phi\colon V\to W$ ein $\mathbb{K}$-Vektorraum.
	\end{defsatz}
	\begin{satz}
		Sind $V$ und $W$ endlichdimensional, dann gilt $\dim\Hom(V, W) = \dim V\cdot\dim W$.
	\end{satz}
	\begin{defsatz}[Dualraum]
		Ist $V$ ein $\mathbb{K}$-Vektorraum, dann ist $V^*\coloneqq\Hom(V,\mathbb{K})$.
		Ist $\set{v_1,\ldots,v_n}$ eine Basis von $V$, dann ist $\set{v_1^*,\ldots,v_n^*}$
		mit $v_j^*(v_k)\coloneqq\delta_{jk}$ eine Basis von $V^*$.
	\end{defsatz}
	\begin{defsatz}[Bidualraum]
		Ist $V$ endlichdimensional und $\dim V \geq 1$, dann ist $V^{**}\coloneqq(V^*)^*\cong V$,
		denn $F\colon V\to V^{**}$ mit $F(x)(\Phi)\coloneqq\Phi(x)$ ist ein Vektorraumisomorphismus.
	\end{defsatz}
	\sep
	\begin{defsatzver}[Abbildungsmatrix]
		Die Abbildung $M^B_C\colon\Hom(V, W)\to \mathbb{K}^{m\times n}$, die jeder
		linearen Abbildung ihre Abbildungsmatrix zuordnet, ist ein Vektorraumisomorphismus.
		Die $k$-te Spalte der Abbildungsmatrix ist $\Theta_C(\Phi(b_k))$.
	\end{defsatzver}
	\begin{satz}
		Für endlichdimensionale $\mathbb{K}$-Vektorräume $V_1$, $V_2$, $V_3$ mit $\dim V_1=l$, $\dim V_2=m$,
		$\dim V_2=n$, Basen $B_1$, $B_2$, $B_3$ und $\Phi\colon V_1\to V_2$ und
		$\Psi\colon V_2\to V_3$ linear gilt $M_{B_3}^{B_1}(\Psi\circ\Phi) = M_{B_3}^{B_2}(\Psi)\cdot M_{B_2}^{B_1}(\Phi)$.
	\end{satz}
	\begin{satz}
		Für endlichdimensionale $\mathbb{K}$-Vektorräume $V$, $W$ mit Basen $B$, $C$ ist
		$\Phi\colon V\to W$ genau dann ein Isomorphismus, wenn $M_C^B(\Phi)$ invertierbar.
		Dann $M_B^C(\Phi^{-1}) = (M_C^B(\Phi))^{-1}$.
	\end{satz}
	\begin{defsatz}[Duale Abbildung]
		Für endlichdimensionale $\mathbb{K}$-Vektorräume $V$, $W$ und $\Phi\colon V\to W$
		linear ist $\Phi^*\colon W^*\to V^*$ mit $\Phi^*(\alpha)\coloneqq\alpha\circ\Phi$
		ebenfalls linear und es gilt $M_{B^*}^{C^*}(\Phi^*) = \transpose{M_C^B(\Phi)}$.
	\end{defsatz}
	\begin{defsatz}
		$\hat{\Phi}\colon\mathbb{K}^n\to\mathbb{K}^m$ ist gegeben durch
		$\Theta_C^{-1}\circ\hat{\Phi}\circ\Theta_B=\Phi$ und $\hat{\Phi}=\Theta_C\circ\Phi\circ\Theta_B^{-1}$.
	\end{defsatz}
	\begin{satz}
		Für $\Phi\colon V\to W$ linear und beliebige Basen $B$, $C$ gilt $\Rang\Phi=\Rang M_C^B$.
	\end{satz}
	\sep
	\begin{satz}
		Für $\Phi\colon V\to W$ linear und Basen $B$, $\tilde{B}$ von $V$ und
		$C$, $\tilde{C}$ von $W$ gilt $M_{\tilde{C}}^{\tilde{B}}(\Phi) = M_{\tilde{C}}^C(\id_W)\cdot M_C^B(\Phi)\cdot M_B^{\tilde{B}}(\id_V)$.
	\end{satz}
	\begin{defsatz}[Äquivalente Matrizen]
		Matrizen $A, \tilde{A} \in \mathbb{K}^{m\times n}$ heißen äquivalent, wenn es
		invertierbare Matrizen $S \in \mathbb{K}^{n\times n}$ und $T \in \mathbb{K}^{m\times m}$
		gibt mit $\tilde{A}=TAS$. Äquivalenz ist eine Äquivalenzrelation und elementare
		Zeilen- und Spaltenumformungen geben eine äquivalente Matrix. Zwei Matrizen sind
		genau dann äquivalent, wenn sie den gleichen Rang haben.
	\end{defsatz}
	\begin{definition}[Ähnliche Matrizen]
		Matrizen $A, \tilde{A} \in \mathbb{K}^{n\times n}$ heißen ähnlich, wenn es eine
		invertierbare Matrix $T\in \mathbb{K}^{n\times n}$ mit $\tilde{A} = TAT^{-1}$ gibt.
	\end{definition}
	\sep
	\begin{satz}
		Das LGS $Ax = b$ ist genau dann lösbar, wenn $\Rang A = \Rang(A \mid b)$.
	\end{satz}
	\begin{satz}
		Ist $Ax = \Phi(x) = b$ lösbar und ist $x_0$ eine beliebige Lösung, dann
		ist $\mathcal{L} = x_0 + \mathcal{L}_h = x_0 + \Kern\Phi$.
	\end{satz}
	\begin{satz}
		Ein homogenes LGS ist genau dann nichttrivial lösbar, wenn $\Rang A < n$.
		Es ist $\dim\mathcal{L}_h = n - \Rang A$.
	\end{satz}
	\sep
	\begin{defsatz}[Determinante]
		Die Funktion $D$, die geordneten $n$-Tupeln aus $\mathbb{K}^n$ einen Skalar aus
		$\mathbb{K}$ zuordnet und die Eigenschaften
		\begin{description}
			\item[D1] $\forall\lambda\in\mathbb{K},1\leq i\leq n: D(a_1,\ldots,\lambda a_i,\ldots,a_n) = \lambda D(a_1,\ldots,a_i,\ldots,a_n)$
			\item[D2] $D(a_1,\ldots,a_i'+a_i'',\ldots,a_n) = D(a_1,\ldots,a_i',\ldots,a_n) + D(a_1,\ldots,a_i'',\ldots,a_n)$
			\item[D3] Aus $a_i = a_j$ für $i \neq j$ folgt $D(a_1,\ldots,a_i,\ldots,a_j,\ldots,a_n) = 0$
			\item[D4] Für die Standardbassis $e_1,\ldots,e_n$ von $\mathbb{K}^n$ ist $D(e_1,\ldots,e_n) = 1$
		\end{description}
		erfüllt, heißt Determinantenfunktion, existiert und ist eindeutig.
	\end{defsatz}
	\begin{satz}[Eigenschaften der Determinantenfunktion]
		\begin{enumerate}[label=(\alph*)]
			\item
				\[ D(a_1,\ldots,\sum_{k=1}^m\lambda_kb_k,\ldots,a_n)=\sum_{k=1}^m\lambda_kD(a_1,\ldots,b_k,\ldots,a_n) \]
			\item
				\[ D(a_1,\ldots,a_i+\sum_{\substack{k=1\\k\neq i}}^m\lambda_ka_k,\ldots,a_n) = D(a_1,\ldots,a_i,\ldots,a_n) \]
			\item
				\[ D(a_1,\ldots,a_i,\ldots,a_j,\ldots,a_n) = -D(a_1,\ldots,a_j,\ldots,a_i,\ldots,a_n) \]
			\item
				\[ D(a_{\sigma(1)},\ldots,a_{\sigma(n)}) = \sign\sigma D(a_1,\ldots,a_n) \]
			\item Die Determinante linear abhängiger Vektoren ist $0$.
		\end{enumerate}
	\end{satz}
	\begin{satz}[Formel von Leibniz]
		\[ D(a_1,\ldots,a_n) = \sum_{\sigma\in S_n}\sign\sigma\cdot a_{1\sigma(1)}\cdots a_{n\sigma(n)} \]
	\end{satz}
	\begin{definition}[Determinante einer quadratischen Matrix]
		$\det A\coloneqq D(a_1,\ldots,a_n)$. Die $a_i$ sind per Definition Zeilenvektoren, das spielt aber keine Rolle.
	\end{definition}
	\begin{satz}
		\begin{enumerate}[label=(\alph*)]
			\item $\det A = \det \transpose{A}$
			\item $\det(A\cdot B) = \det A\cdot \det B$
			\item Eine quadratische Matrix $A$ ist genau dann regulär, wenn $\det A\neq 0$. Dann $\det A^{-1} = (\det A)^{-1} = \frac{1}{\det A}$.
		\end{enumerate}
	\end{satz}
	\begin{satzver}[Berechnung der Determinante]
		\begin{enumerate}[label=(\alph*)]
			\item Entwicklung nach der $k$-ten Spalte:
				\[ D(a_1,\ldots,a_n) = \sum_{j=1}^n(-1)^{k+j}a_{jk}D_{jk} \]
			\item Entwicklung nach der $i$-ten Zeile:
				\[ D(a_1,\ldots,a_n) = \sum_{k=1}^n(-1)^{i+k}a_{ik}D_{ik} \]
			\item Beim Vertauschen von zwei Zeilen muss mit $-1$ multipliziert werden.
			\item Wird eine Zeile mit $k$ multipliziert, muss mit $k^{-1}$ multipliziert werden.
			\item Wird das Vielfache einer Zeile auf eine andere Zeile addiert, wird nicht multipliziert.
		\end{enumerate}
	\end{satzver}
	\sep
	\begin{satz}
		Ein homogenes LGS $Ax=0$ hat genau dann nur die triviale Lösung, wenn $\det A\neq0$.
	\end{satz}
	\begin{satz}[Kramersche Regel]
		Ein LGS $Ax=b$ mit $\det A\neq 0$ hat genau eine Lösung $x = (x_1,\ldots,x_n)$ und es gilt
		$x_k = \frac{D_k}{\det A}$, wobei $D_k\coloneqq\det(a_{\cdot1}\cdots a_{\cdot k} \mid b \mid a_{\cdot k+1}\cdots a_{\cdot n})$.
	\end{satz}
	\sep
	\begin{definition}[Eigenwert]
		Für $\Phi\colon V\to V$ heißt $\lambda\in\mathbb{K}$ Eigenwert, wenn $0\neq x\in V$
		existiert mit $\Phi(x)=\lambda x \iff (\Phi-\lambda\id_V)(x)=0$.
	\end{definition}
	\begin{definition}[Eigenraum]
		$E_\lambda\coloneqq\set{x\given\Phi(x)=\lambda x}=\Kern(\Phi-\lambda\id_V)$
	\end{definition}
	\begin{definition}[Spektrum]
		$\set{\lambda\given\exists x\neq0:\Phi(x)=\lambda x}$ heißt Spektrum von $\Phi$.
	\end{definition}
	\begin{satz}
		Zu verschiedenen Eigenwerten gehörende Eigenvektoren eines Endomorphismus sind linear unabhängig.
		Ein Endomorphismus hat somit maximal $n$ Eigenwerte.
	\end{satz}
	\begin{definition}[Charakteristisches Polynom]
		$p_A\coloneqq\det(A-XE_n)$
	\end{definition}
	\begin{satzver}[Bestimmung der Eigenwerte]
		$\lambda~\text{ist Eigenwert von $A \in\mathbb{K}^{n\times n}$} \iff \det(A-\lambda E_n) = 0 \iff \lambda~\text{ist Nullstelle von $p_A$}$.
	\end{satzver}
	\begin{satzver}[Bestimmung des Eigenraums]
		$E_\lambda$ entspricht der Lösungsmenge des homogenen LGS $(A-\lambda E_n)x = 0$.
	\end{satzver}
	\sep
	\begin{definition}[Diagonalisierbar]
		Eine quadratische Matrix heißt diagonalisierbar, wenn sie zu einer Diagonalmatrix
		ähnlich ist. Ein Endomorphismus heißt diagonalisierbar, wenn er eine Abbildungsmatrix
		in Diagonalgestalt hat.
	\end{definition}
	\begin{definition}[Geometrische Vielfalt]
		Die Geometrische Vielfalt eines Eigenwerts $\lambda$ ist $\dim\Kern(\Phi-\lambda\id_V)$.
	\end{definition}
	\begin{definition}[Linearfaktoren und algebraische Vielfalt]
		Ein $p_A$ zerfällt in Linearfaktoren, wenn es sich
		in der Form $p_A=(-1)^n(X-\lambda_1)^{r_1}\cdots(X-\lambda_k)^{r_k}$ schreiben
		lässt. $r_i$ ist dann die algebraische Vielfalt des Eigenwerts $\lambda_i$.
	\end{definition}
	\begin{satz}
		Die folgenden Aussagen sind äquivalent.
		\begin{enumerate}[label=(\alph*)]
			\item $\Phi$ ist diagonalisierbar.
			\item In $V$ gibt es eine Basis aus Eigenvektoren von $\Phi$.
			\item $V$ ist die direkte Summe der Eigenräume von $\Phi$.
			\item Die Summe der Dimensionen der Eigenräume von $\Phi$ ist $n$.
			\item Das charakteristische Polynom zerfällt in Linearfaktoren und
			für alle Eigenwerte sind die geometrische und algebraische Vielfalt gleich.
		\end{enumerate}
	\end{satz}
	\begin{defsatz}[Trigonalisierbarkeit]
		Ein Endomorphismus heißt trigonalisierbar, wenn er eine Abbildungsmatrix hat,
		die eine obere Dreiecksmatrix ist. Dies ist genau dann der Fall, wenn das
		charakterisitische Polynom in Linearfaktoren zerfällt.
	\end{defsatz}
	\sep
	\begin{defsatz}[Einsetzungshomomorphismus]
		Für $p = a_0 + a_1X + \ldots + a_nX^n \in\mathbb{K}[X]$ ist $p(\Phi)\coloneqq a_0\id_V+a_1\Phi+\ldots+a_n\Phi^n \in\Hom(V, V)$.
		Die Abbildung $\mathbb{K}[X]\to\Hom(V, V)$ mit $p\mapsto p(\Phi)$ ist ein Ringhomomorphismus und eine lineare Abbildung.
	\end{defsatz}
	\begin{satz}[von Cayley-Hamilton]
		Für einen Endomorphismus $\Phi$ ist $p_\Phi(\Phi) = 0$, also $\forall v\in V: p_\Phi(\Phi)(v) = 0$.
	\end{satz}
	\newpage
	\textsc{Lineare Algebra II}

	\sep
	\begin{verfahren}[Jordansche Normalform]
		Es sei $A\in\mathbb{C}^{n\times n}$. Für jedes $\lambda\in\Spek A$:
		\begin{enumerate}
			\item Bestimme das minimale $q$ mit $\Kern(A-\lambda E_n)^q = \Kern(A-\lambda E_n)^{q+1}$
			durch wiederholte Matrixmultiplikation und Gauß.
			\item Die Zahl der $k$-Kästchen: $m_k = \dim\Kern(A-\lambda E_n)^k - \dim\Kern(A-\lambda E_n)^{k - 1} - \sum_{i=k+1}^q m_i$
			(rückwärts ausrechnen).
			\item Wieder rückwärts: $m_k$ Basisvektoren $b_1, \ldots, b_{m_k}$ wählen, die in $\Kern(A-\lambda E_n)^k \setminus \Kern(A-\lambda E_n)^{k - 1}$
			liegen und zu den anderen Vektoren auf dieser Ebene linear unabhängig sind. Für $i\in\set{1,\ldots,m_k}$ sind dann
			$b_i, (A-\lambda E_n)(b_i), \ldots, (A-\lambda E_n)^{k - 1}(b_i)$ gesuchte Basisvektoren.
		\end{enumerate}
	\end{verfahren}
	\sep
	\begin{definition}[Bilinearform, Skalarprodukt, euklidischer Vektorraum]
		Es sei $V$ ein reeller Vektorraum.
		\begin{enumerate}[label=(\alph*)]
			\item Eine Abbildung $F\colon V\times V\to\mathbb{R}$ mit
				$\forall a, a_1, a_2, b, b_1, b_2\in V, \lambda_1, \lambda_2, \mu_1, \mu_2\in\mathbb{R}\colon$
				\begin{align*}
					F(\lambda_1 a_1+\lambda_2 a_2, b) &= \lambda_1 F(a_1, b) + \lambda_2 F(a_2, b)\\
					F(a, \mu_1 b_1 + \mu_2 b_2) &= \mu_1 F(a, b_1) + \mu_2 F(a, b_2)
				\end{align*}
				heißt Bilinearform auf $V$.
			\item Eine Bilinearform $F$ mit $\forall a, b\in V\colon F(a, b) = F(b, a)$ heißt symmetrisch.
			\item Eine Bilinearform $F$ mit $\forall a\in V\colon F(a, a)\geq0$ und $F(a, a)=0\iff a = 0$ heißt
				positiv definit.
			\item Eine positiv definitie, symmetrische Bilinearform heißt Skalarprodukt auf $V$.
			\item Ein reeller Vektorraum $V$ zusammen mit einem Skalarprodukt $F$ heißt euklidischer Vektorraum.
		\end{enumerate}
	\end{definition}
	\begin{definition}[Hermitesche Form, Skalarprodukt, unitärer Vektorraum]
		Es sei $V$ ein komplexer Vektorraum.
		\begin{enumerate}[label=(\alph*)]
			\item Eine Abbildung $F\colon V\times V\to\mathbb{C}$ mit
				$\forall a, a_1, a_2, b\in V, \lambda_1, \lambda_2\in\mathbb{C}\colon$
				\begin{align*}
					F(\lambda_1 a_1+\lambda_2 	a_2, b) &= \lambda_1 F(a_1, b) + \lambda_2 F(a_2, b)\\
					F(b, a) = \conj{F(a, b)}
				\end{align*}
				heißt hermitesche Form auf $V$.
			\item Eine hermitesche Form $F$ mit $\forall a\in V\colon F(a, a)\geq 0$ und $F(a, a) = 0\iff a = 0$
				heißt positiv definit.
			\item Eine positiv definite hermitesche Form heißt Skalarprodukt auf $V$.
			\item Ein komplexer Vektorraum $V$ zusammen mit einem Skalarprodukt $F$ heißt unitärer Vektorraum.
		\end{enumerate}
	\end{definition}
	\begin{satz}[Eigenschaften hermitescher Formen]
		Es sei $F$ eine hermitesche Form auf einem komplexen Vektorraum $V$. Für
		$a, b_1, b_2\in V, \mu_1, \mu_2\in\mathbb{C}$ gilt:
		\begin{enumerate}[label=(\alph*)]
			\item $F(a, \mu_1 b_1+ \mu_2 b_2) = \conj{\mu_1} F(a, b_1) + \conj{\mu_2} F(a, b_2)$
			\item $F(a, a)\in\mathbb{R}$
			\item $F(0, 0) = 0$
		\end{enumerate}
	\end{satz}
	\begin{defsatz}[Matrix von Skalarprodukten]
		Ist $(V, \scp{})$ ein $n$-dimensionaler euklidischer oder unitärer
		Vektorraum mit geordneter Basis $B = \set{b_1,\ldots,b_n}$, so wird durch
		$g_{kl}\coloneqq \scp{b_k, b_j}$ die Matrix $G$ des Skalarprodukts definiert.
		Es ist $G = \transpose{\conj{G}}$ und $\scp{a, b} = \transpose{\Theta_B(a)}G\conj{\Theta_B(b)}$
		für alle $a, b\in V$.
	\end{defsatz}
	\begin{definition}
		\begin{enumerate}[label=(\alph*)]
			\item $A\in\mathbb{R}^{n\times n}$ mit $\transpose{A} = A$ heißt symmetrisch.
			\item $A\in\mathbb{C}^{n\times n}$ mit $\transpose{\conj{A}} = A$ heißt hermitesch.
			\item Symmetrisches oder hermitesches $A$ mit $\forall z\in\mathbb{C}^n\setminus\set{0}:
				\transpose{z}A\conj{z}>0$ heißt positiv definit.
		\end{enumerate}
	\end{definition}
	\begin{satz}
		Die Matrix eines Skalarproduktes ist positiv definit. Jede positiv definitierte
		Matrix induziert Skalarprodukte auf alle $n$-dimensionalen reellen bzw. komplexen
		Vektorräume.
	\end{satz}
	\begin{satz}[Basiswechsel für Skalarprodukte]
		Ist $V$ ein $n$-dimensionaler euklidischer oder unitärer Vektorraum mit
		geordneten Basen $B$ und $\tilde{B}$, sind $G$ und $\tilde{G}$ die Matrizen
		eines Skalarprodukts $\scp{}$ bezüglich $B$ bzw. $\tilde{B}$, und ist $S$ die Basiswechselmatrix
		von $\tilde{B}$ nach $B$, so gilt $\tilde{G} = \transpose{S}G\conj{S}$.
	\end{satz}
	\sep
	\begin{satz}[Cauchy-Schwarz]
		Für $(V, \sp{})$ euklidisch oder unitär, $a, b\in V$ gilt $\abs{\scp{a, b}}^2\leq\scp{a, a}\scp{b, b}$.
		Gleichheit genau dann, wenn $a$ und $b$ linear abhängig.
	\end{satz}
	\begin{definition}[Norm]
		Es sei $V$ reell oder komplex. $\norm{}\colon V\to\mathbb{R}$ mit
		$\forall\lambda\in\mathbb{R}\ \text{bzw.}\ \mathbb{C}, a, b\in V\colon$
		\begin{enumerate}[label=(\alph*)]
			\item $\norm{\lambda a} = \abs{\lambda}\norm{a}$
			\item $\norm{a + b} \leq \norm{a} + \norm{b}$
			\item $\norm{a} \geq 0$ und $\norm{a} = 0\iff a = 0$
		\end{enumerate}
		heißt Norm. Ein Vektorraum mit einer Norm heißt normierter Vektorraum.
	\end{definition}
	\begin{satz}
		\begin{enumerate}[label=(\alph*)]
			\item Ist $(V, \scp{})$ euklidisch oder unitär, so ist $\norm{}\colon V\to\mathbb{R}$ mit
				$\forall a\in V\colon\norm{a}\coloneqq\sqrt{\scp{a, a}}$ eine Norm. Dann gilt
				$\forall a, b\in V\colon \norm{a + b}^2 + \norm{a - b}^2 = 2\norm{a}^2 + 2\norm{b}^2$.
			\item Ist $(V, \norm{})$ normiert mit
				$\forall a, b\in V\colon \norm{a + b}^2 + \norm{a - b}^2 = 2\norm{a}^2 + 2\norm{b}^2$,
				so ist durch $\forall a, b\in V\colon \scp{a, b}\coloneqq \frac{1}{2}(\norm{a + b}^2 - \norm{a}^2 - \norm{b}^2)$
				ein Skalarprodukt auf $V$ definiert.
		\end{enumerate}
	\end{satz}
	\begin{definition}[Metrik]
		Es sei $M$ eine Menge. $d\colon M\cross M\to\mathbb{R}$ mit $\forall p, q, r\in M\colon$
		\begin{enumerate}[label=(\alph*)]
			\item $d(p, q) = d(q, p)$
			\item $d(p, r) \leq d(p, q) + d(q, r)$
			\item $d(p, q) \geq 0$ und $d(q, p) = 0 \iff p = q$
		\end{enumerate}
		heißt Metrik. $(M, d)$ ist dann ein metrischer Raum.
	\end{definition}
	\begin{satz}
		Ist $(V, \norm{})$ normiert, so ist $(V, d)$ mit $\forall x, y\in V\colon d(x, y)\coloneqq \norm{x - y}$
		ein metrischer Raum.
	\end{satz}
	\begin{definition}[Winkel]
		Es sei $(V, \scp{})$ euklidisch, $a, b\in V\setminus\set{0}$. Der Winkel
		zwischen $a$ und $b$ ist $\omega(a, b)\in[0,\pi]$ mit $\cos\omega(a, b) = \frac{\scp{a, b}}{\norm{a}\norm{b}}$
		und ist eindeutig bestimmt.
	\end{definition}
	\begin{satz}[Rechenregeln zu Winkeln]
		Es sei $(V, \scp{})$ euklidisch, $a, b\in V\setminus\set{0}$, $\alpha, \beta\in\mathbb{R}\setminus\set{0}$.
		\begin{enumerate}[label=(\alph*)]
			\item $\omega(a, b) = \omega(b, a)$
			\item $\omega(\alpha a, \beta b) =
				\begin{cases}
					\omega(a, b), &\alpha\beta > 0\\
					\pi-\omega(a, b), &\alpha\beta < 0
				\end{cases}$
			\item $\omega(a, b) = 0 \iff \exists\lambda > 0\colon b=\lambda a$
			\item $\omega(a, b) = 0 \iff \exists\lambda < 0\colon b=\lambda a$
		\end{enumerate}
	\end{satz}
	\sep
	\begin{definition}[Orthogonalität, Orthonormiertheit, Orthonormalbasis]
		Ist $(V,\scp{})$ euklidisch oder unitär, $a, b\in V$ und $\scp{a, b}=0$,
		so heißen $a$ und $b$ orthogonal. Man schreibt $a\perp b$. Mengen paarweise
		orthogonaler Vektoren heißen orthogonal. Orthogonale Mengen $S$ mit $\forall v\in S\colon \norm{v}=1$
		heißen orthonormiert. Orthonormierte Basen von $V$ heißen Orthonormalbasen (ONBs).
	\end{definition}
	\begin{satz}[des Pythagoras]
		Ist $(V, \scp{})$ euklidisch oder unitär, so gilt für $a, b\in V$: $a\perp b\implies \norm{a}^2 + \norm{b}^2 = \norm{a+b}^2$.
		Im euklidischen Fall gilt auch die umgekehrte Implikation.
	\end{satz}
	\begin{satzver}[Gram-Schmidt]
		Es sei $(V, \scp{})$ euklidisch oder unitär.
		\begin{enumerate}[label=(\alph*)]
			\item Orthogonale $S\subset V\setminus\set{0}$ sind linear unabhängig.
			\item Ist $S\subset V\setminus\set{0}$ orthogonal, so ist
				$S*\coloneqq\set{\norm{a}^{-1}a\given a\in S}$ orthonormiert.
			\item Ist $\set{a_1,\ldots,a_n}$ eine ONB von $(V,\scp{})$, so gilt
				$\forall v\in V\colon v = \sum_{i=1}^n\scp{v,a_i}a_i$.
			\item Ist $B\coloneqq \set{b_1,\ldots,b_l}$ linear unabhängig in $V$, so wird durch
				\begin{align*}
					a_1 &\coloneqq b_1,\\
					\forall k\in\set{1,\ldots,l - 1}\colon a_{k+1} &\coloneqq b_{k+1} - \sum_{j=1}^k\frac{\scp{b_{k+1}, a_j}}{\scp{a_j, a_j}}a_j
				\end{align*}
				eine orthognale Menge $A\coloneqq\set{a_1,\ldots,a_l}$ mit $[A] = [B]$ definiert.
			\item Ist $V$ endlichdimensional, so hat $V$ eine ONB.
		\end{enumerate}
	\end{satzver}
	\begin{defsatz}[Orthogonalkomplement]
		Ist $(V, \scp{})$ euklidisch oder unitär und $U\subset V$ ein Untervektorraum, so heißt
		$U^\perp \coloneqq\set{x\in V\given\forall u\in U\colon\scp{x, u}=0}$ das Orthogonalkomplement
		von $U$.
		\begin{enumerate}[label=(\alph*)]
			\item $U^\perp$ ist ein Untervektorraum von $V$
			\item $U\cap U^\perp=\set{0}$
			\item Ist V endlichdimensional, so ist $U\oplus U^\perp = V$.
		\end{enumerate}
	\end{defsatz}
	\begin{defsatzver}[Orthogonalprojektion]
		Es sei $(V, \scp{})$ euklidisch oder unitär und $U\subset V$ ein Untervektorraum.
		$\pi_U\colon V\to U$ mit $\pi_U(u+u^\perp)\coloneqq u$, wobei $u\in U, u^\perp\in U^\perp$, heißt Orthogonalprojektion auf $U$.
		\begin{enumerate}[label=(\alph*)]
			\item $\pi_U$ ist linear
			\item $\pi_U^2 = \pi_U$
			\item $\Bild\pi_U = U$, $\Kern\pi_U = U^\perp$
			\item $\forall v, w\in V\colon d(\pi_U(v), \pi_U(w)) \leq d(v, w)$
			\item Ist $\set{e_1,\ldots,e_k}$ eine ONB von $U$, so gilt $\forall v\in V\colon \pi_U(v)=\sum_{j=1}^k\scp{v, e_j}e_j$
		\end{enumerate}
	\end{defsatzver}
	\begin{defsatzver}[Abstand von einem Untervektorraum]
		Ist $(V, \scp{})$ euklidisch oder unitär, $v\in V$ und $U$ Untervektorraum von $V$, so ist
		$d(\set{v}, U)\coloneqq \inf\set{\norm{v-u}\given u\in U}$. Ist $V$ zusätzlich endlichdimensional
		so gilt $d(\set{v}, U) = \norm{\pi_{U^\perp}(v)}$.
	\end{defsatzver}
	\sep
	\begin{defsatz}
		$A\in\mathbb{R}^{n\times n}$ mit $\transpose{A}A=E_n$ heißt orthogonal.
		$A\in\mathbb{C}^{n\times n}$ mit $\transpose{A}\conj{A}=E_n$ heißt unitär.

		Die orthogonalen bzw. unitären Matrizen sind genau die Matrizen, die ONBs in
		ONBs überführen.

		Für $A$ orthognal oder unitär gilt $\abs{\det A} = 1$.
	\end{defsatz}
	\begin{satz}
		Es sind äquivalent:
		\begin{enumerate}[label=(\alph*)]
			\item $A$ ist orthogonal/unitär
			\item $A$ ist regulär und $A^{-1}=\transpose{\conj{A}}$
			\item Die Spalten- und Zeilenvektoren von A bilden eine ONB von
				$\mathbb{R}^n$ bzw. $\mathbb{C}^n$ bezüglich des Standardskalarprodukts.
		\end{enumerate}
	\end{satz}
	\begin{defsatz}
		Es sei $n\in\mathbb{N}$.
		\begin{align*}
			\mathbf{O}(n) &\coloneqq \set{A\in\mathbf{GL}(n,\mathbb{R})\given \transpose{A}A=E_n}\\
			\mathbf{U}(n) &\coloneqq \set{A\in\mathbf{GL}(n,\mathbb{C})\given \transpose{\conj{A}}A=E_n}\\
			\mathbf{SL}(n, \mathbb{C}) &\coloneqq \set{A\in\mathbb{C}^{n\times n}\given\det A=1}\\
			\mathbf{SL}(n, \mathbb{R}) &\coloneqq \mathbf{SL}(n, \mathbb{C})\cap\mathbf{GL}(n, \mathbb{R})\\
			\mathbf{SO}(n) &\coloneqq \mathbf{O}(n)\cap\mathbf{SL}(n,\mathbb{R})\\
			\mathbf{SU}(n) &\coloneqq \mathbf{U}(n)\cap\mathbf{SL}(n,\mathbb{C})
		\end{align*}
	\end{defsatz}
	\begin{satz}[Iwasawa-Zerlegung]
		Nope.
	\end{satz}
	\sep
	\begin{definition}[Adjungierte Abbildung]
		Es seien $(V, \scp{}_V)$ und $(W, \scp{}_W)$ euklidisch oder unitär und
		$\Phi\colon V\to\ W$ linear. $\Phi^*\colon W\to V$ heißt zu $\Phi$ adjungiert,
		wenn $\forall v\in V, w\in W\colon \scp{\Phi(v), w}_W = \scp{v, \Phi^*(w)}_V$.
	\end{definition}
	\begin{definition}
		$\forall A\in\mathbb{C}^{m\times n}\colon A^*\coloneqq \transpose{\conj{A}}$.
	\end{definition}
	\begin{satz}
		Es seien $(V, \scp{})$, $(W, \scp)$ euklidisch oder unitär.
		\begin{enumerate}[label=(\alph*)]
			\item Ist $\dim V<\infty$, so existiert für jedes $\Phi\colon V\to W$ genau
				Adjungierte $\Phi^*\colon W\to V$.
			\item Ist $\dim V<\infty$ und $\dim W<\infty$, $\Phi\colon V\to W$ linear,
				$B$ eine ONB von $V$, $C$ eine ONB von $W$, so gilt $M^C_B(\Phi^*) = M^B_C(\Phi)^*$.
		\end{enumerate}
	\end{satz}
	\begin{satz}[Rechenregeln für Adjungierte]
		Es seien $(U, \scp{})$, $(V, \scp{})$, $(W, \scp{})$ euklidisch oder unitär,
		$\Phi\colon U\to V$ mit Adjungierter $\Phi^*\colon V\to U$, $\Psi\colon V\to W$
		mit Adjungierter $\Psi^*\colon W\to V$. Dann:
		\begin{enumerate}[label=(\alph*)]
			\item $\forall \lambda\in\mathbb{C}\colon (\lambda\Phi)^* = \conj{\lambda}\Phi^*$
			\item $(\Psi\circ\Phi)^* = \Phi^*\circ\Psi^*$
			\item $\Phi^{**} = (\Phi^*)^* = \Phi$
			\item $\Kern\Phi^* = (\Bild\Phi)^\perp$
		\end{enumerate}
	\end{satz}
	\sep
	\begin{defsatz}[Selbstadjungierte Abbildung]
		Ist $(V,\scp{})$ euklidisch oder unitär und endlichdimensional, so heißt $\Phi\colon V\to V$ selbstadjungiert,
		wenn $Phi^*=\Phi$. Dies ist genau dann der Fall, wenn die Abbildungsmatrix von $\Phi$ bezüglich einer
		ONB von $V$ symmetrisch bzw. hermitesch ist.
	\end{defsatz}
	\begin{satz}
		Ist $(V, \scp{})$ euklidisch oder unitär mit $\dim V=n\in\mathbb{N}$ und $\Phi\colon V\to V$ selbstadjungiert,
		so gilt \[p_\Phi=\pm(X-\lambda_1)\cdots(X-\lambda_n)\] mit $\forall i\in\set{1,\ldots,n}\colon\lambda_i\in\mathbb{R}$.
	\end{satz}
	\begin{satz}[Spektralsatz]
		\begin{enumerate}[label=(\alph*)]
			\item Es sei $(V, \scp{})$ euklidisch oder unitär mit $\dim V=n\in\mathbb{N}$ und $\Phi\colon V\to V$ selbstadjungiert.
				Dann existiert eine ONB $B$ von $V$, bezüglich der $M_B^B(\Phi)$ Diagonalgestalt annimmt. Insbesondere ist $\Phi$
				also diagonalisierbar.
			\item Ist $A\in\mathbb{R}^{n\times n}$ symmetrisch, so existiert
				$S\in\mathbf{O}(n)$ mit $S^*AS=S^{-1}AS = \diag(\lambda_1,\ldots,\lambda_n)$.
			\item Ist $A\in\mathbb{C}^{n\times n}$ hermitesch, so existiert
				$S\in\mathbf{U}(n)$ mit $S^*AS=S^{-1}AS = \diag(\lambda_1,\ldots,\lambda_n)$,
				und es ist $\forall i\in\set{1,\ldots,n}\colon \lambda_i\in\mathbb{R}$.
			\item Die Eigenvektoren eines selbstadjungierten Endomorphismus zu unterschiedlichen
				Eigenwerten sind orthogonal.
			\item Es sei $(V,\scp{})$ euklidisch oder unitär. $\Phi\colon V\to V$ sei selbstadjungiert
				mit den unterschiedlichen Eigenwerten $\lambda_1,\ldots,\lambda_r$. Dann ist
				$V=\bigoplus_{j=1}^rE_{\lambda_j}$ und $\Phi = \lambda_1\pi_{E_{\lambda_1}}+\ldots+\lambda_r\pi_{E_{\lambda_r}}$.
		\end{enumerate}
	\end{satz}
	\begin{satz}[Kriterium für positiv definit]
		Eine reelle, symmetrische Matrix $A$ ist genau dann positiv definit, wenn alle
		Eigenwerte von $A$ positiv sind.
	\end{satz}
	\begin{satz}
		Ist $A\in\mathbb{R}^{n\times n}$ reell, symmetrisch und positiv definit,
		so existiert $\sqrt{A}\in\mathbb{R}^{n\times n}$ mit $\sqrt{A}\sqrt{A}=A$.
	\end{satz}
\end{document}
